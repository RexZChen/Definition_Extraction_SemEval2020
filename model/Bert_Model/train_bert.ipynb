{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF9gDZL3Vf7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "751a35e8-d033-4597-dc2c-abfddf561881"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.1\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=bb45271ce1ab7bf29970f1bbdaeeeff26e73269e8b105cd7c9de34c2d12bac9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Y51LALV4AE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "b84f0e4c-be08-4bc3-f932-f015195d0a64"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGCBhZEWqjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bert_DataFetcher import BertDataSet\n",
        "from Capsule_Bert_model import BertCapsuleNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94LAzWjYWgPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\"\n",
        "\n",
        "ds = BertDataSet('train.txt')\n",
        "ds_loader = DataLoader(ds, batch_size=16, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 2e-3  # 2e-3\n",
        "max_grad_norm = 1.0\n",
        "num_training_steps = len(ds.data_x) * 10\n",
        "num_warmup_steps = 0\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
        "\n",
        "\n",
        "device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "\n",
        "model = BertCapsuleNet(freeze_bert=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                 )\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2htsMkBXuzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "cd6ee41c-8b68-431b-fc85-13e6cb9cbe3d"
      },
      "source": [
        "# train\n",
        "\n",
        "num_epoch = 8\n",
        "\n",
        "model.train()\n",
        "\n",
        "for e in range(num_epoch):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(ds_loader):\n",
        "        X, Y, attn_msks = data\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "        attn_msks = attn_msks.to(device)\n",
        "        outputs = model((X, attn_msks))\n",
        "        loss = criterion(outputs, Y)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        # print statistics\n",
        "        if i % 200 == 199:\n",
        "            print('%d epoch: %d Done, loss = %f' % (e, i, running_loss / 200.0))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"------Finish training------\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch: 199 Done, loss = 0.460854\n",
            "0 epoch: 399 Done, loss = 0.394472\n",
            "0 epoch: 599 Done, loss = 0.379074\n",
            "0 epoch: 799 Done, loss = 0.375714\n",
            "0 epoch: 999 Done, loss = 0.375161\n",
            "1 epoch: 199 Done, loss = 0.269943\n",
            "1 epoch: 399 Done, loss = 0.275934\n",
            "1 epoch: 599 Done, loss = 0.271563\n",
            "1 epoch: 799 Done, loss = 0.257526\n",
            "1 epoch: 999 Done, loss = 0.269604\n",
            "2 epoch: 199 Done, loss = 0.180164\n",
            "2 epoch: 399 Done, loss = 0.182894\n",
            "2 epoch: 599 Done, loss = 0.210615\n",
            "2 epoch: 799 Done, loss = 0.197214\n",
            "2 epoch: 999 Done, loss = 0.200234\n",
            "3 epoch: 199 Done, loss = 0.132437\n",
            "3 epoch: 399 Done, loss = 0.138663\n",
            "3 epoch: 599 Done, loss = 0.154227\n",
            "3 epoch: 799 Done, loss = 0.186309\n",
            "3 epoch: 999 Done, loss = 0.159431\n",
            "4 epoch: 199 Done, loss = 0.105363\n",
            "4 epoch: 399 Done, loss = 0.119837\n",
            "4 epoch: 599 Done, loss = 0.138817\n",
            "4 epoch: 799 Done, loss = 0.136771\n",
            "4 epoch: 999 Done, loss = 0.127079\n",
            "5 epoch: 199 Done, loss = 0.085313\n",
            "5 epoch: 399 Done, loss = 0.101960\n",
            "5 epoch: 599 Done, loss = 0.100628\n",
            "5 epoch: 799 Done, loss = 0.115604\n",
            "5 epoch: 999 Done, loss = 0.125057\n",
            "6 epoch: 199 Done, loss = 0.085104\n",
            "6 epoch: 399 Done, loss = 0.091059\n",
            "6 epoch: 599 Done, loss = 0.082209\n",
            "6 epoch: 799 Done, loss = 0.081294\n",
            "6 epoch: 999 Done, loss = 0.101398\n",
            "7 epoch: 199 Done, loss = 0.053592\n",
            "7 epoch: 399 Done, loss = 0.077095\n",
            "7 epoch: 599 Done, loss = 0.079102\n",
            "7 epoch: 799 Done, loss = 0.084410\n",
            "7 epoch: 999 Done, loss = 0.075988\n",
            "------Finish training------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbS_HaXbIA6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c719f4b3-a32c-4ae7-a44b-e83d632e18fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKxy6P2HH-Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"./gdrive/My Drive/bert_capsule.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVzi77vN19Ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a7c4d4a-d043-4455-db0d-18cd33c5c448"
      },
      "source": [
        "# test\n",
        "\n",
        "model = BertCapsuleNet(freeze_bert=False)\n",
        "model.load_state_dict(torch.load(\"./gdrive/My Drive/bert_capsule.pt\"))\n",
        "model = model.to(device)\n",
        "\n",
        "test_ds = BertDataSet('test.txt')\n",
        "test_ds_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# matrix used for computing f1 score\n",
        "y_true = None\n",
        "y_pred = None\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_ds_loader):\n",
        "        X, labels, attn_msks = data\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "        attn_msks = attn_msks.to(device)\n",
        "        outputs = model((X, attn_msks))\n",
        "        _, predicted = torch.max(outputs.data, 1)  # predicted shape: [batch_size, 1]\n",
        "        total += labels.size(0)  # labels shape: [batch_size, 1]\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        if i == 0:\n",
        "            y_true = labels\n",
        "            y_pred = predicted\n",
        "        else:\n",
        "            y_true = torch.cat((y_true, labels), 0)\n",
        "            y_pred = torch.cat((y_pred, predicted), 0)\n",
        "    print('F1 score: ', f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy()))\n",
        "    print('Precision score: ', precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy()))\n",
        "    print('Recall score: ', recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy()))\n",
        "    print('Accuracy score: ', accuracy_score(y_true.cpu().numpy(), y_pred.cpu().numpy()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score:  0.7962962962962964\n",
            "Precision score:  0.8052434456928839\n",
            "Recall score:  0.7875457875457875\n",
            "Accuracy score:  0.8641975308641975\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}